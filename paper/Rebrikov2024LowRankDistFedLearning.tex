\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[T1,T2A]{fontenc}
\usepackage[english, russian]{babel}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\usepackage{macros}



\title{Методы малоранговых разложений в распределенном и федеративном обучении}

\author{ Ребриков Алексей \\
	\texttt{rebrikov.av@phystech.edu} \\
	%% examples of more authors
	\And
	Зыль Александр\\
	% \texttt{beznosikov.an@phystech.edu} \\
	\And 
	Безносиков Александр\\
	\texttt{beznosikov.an@phystech.edu} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}
\renewcommand{\undertitle}{}

\hypersetup{
pdftitle={Методы малоранговых разложений в распределенном и федеративном обучении},
pdfsubject={Малоранговые разложения},
pdfauthor={Ребриков~А.В., Безносиков~А.Н., Зыль~А.В.},
pdfkeywords={сжатие информации, малоранговые разложения, распределенное обучение, федеративное обучение},
}

\begin{document}
\maketitle

\begin{abstract}
	Подходы распределенного и федеративного обучения становятся все более популярными в обучении современных SOTA моделей машинного обучения. При этом на первый план выходит вопрос организации эффективных коммуникаций, так как процесс передачи информации занимает слишком много времени даже в случае кластерных вычислений. Из-за этого может теряться смысл в распределении/распараллеливании процесса обучения. Одной из ключевой техник  борьбы с коммуникационными затратами является использование сжатий передаваемой информации. На данный момент в литературе предлагаются различные техники сжатия (\citep{beznosikov2023biased}, \citep{alistarh2017qsgd}, \citep{horvoth2022natural}), но потенциал в этом вопросе явно не исчерпан. В частности, довольно большой потенциал кроется в малоранговых разложениях \citep{Gundersen2019RSVD}. В рамках проекта предлагается сконструировать операторы сжатия на основе данных разложений и встроить в методы распределенной оптимизации \citep{richtarik2021ef21}.
\end{abstract}


\keywords{сжатие информации \and малоранговые разложения \and распределенное обучение \and федеративное обучение}

\section{Введение}

Цель данного исследования заключается в разработке и анализе методов малоранговых разложений для сжатия информации в контексте распределенного и федеративного обучения. Мотивация исследования проистекает из растущей потребности в эффективных методах обучения для современных масштабных моделей машинного обучения, где коммуникационные затраты становятся критическим барьером для эффективности. Объектом исследования являются операторы сжатия, основанные на малоранговых разложениях, и их интеграция в методы распределенной оптимизации. 

Проводится обзор существующей литературы и анализируются последние достижения в области сжатия информации для распределенного обучения. В частности, рассматриваются существующие техники сжатия, такие как предложенные в работах \citep{beznosikov2023biased}, \citep{alistarh2017qsgd}, и \citep{horvoth2022natural}, а также исследуется потенциал малоранговых разложений.

Задачами проекта являются разработка операторов сжатия на основе малоранговых разложений, их интеграция в алгоритмы распределенной оптимизации и оценка влияния на эффективность обучения. Предлагаемое решение предполагает новизну в виде конкретной реализации сжатия, которая потенциально позволяет уменьшить коммуникационные затраты без значительной потери качества обучения.

Цель эксперимента состоит в демонстрации эффективности предлагаемых методов на реальных наборах данных и в различных условиях обучения, оценке улучшения скорости и качества обучения.

\section{Определение оптимизационной задачи и ее решение}
Для достижения высоких результатов современные модели машинного обучения тренируются на больших наборах данных, что часто требует обширного числа обучаемых параметров. Рассматриваем задачи оптимизации вида
\begin{align}\label{eq:prob}
\min \limits_{x \in \R^d} \left\{ f(x) \eqdef \frac{1}{n} \sum \limits_{i=1}^n f_i(x) \right\},
\end{align}
где $x \in \R^d$ представляет параметры модели, $n$ — количество работников/устройств, а $f_i(x)$ — функции потерь модели $x$ на данных, хранимых на устройстве $i$. Функция потерь $f_i: \R^d \to \R$ часто имеет вид $$f_i(x) \eqdef \EE{\xi \sim \cP_i}{f_\xi(x)},$$ где $\cP_i$ обозначает распределение данных обучения, принадлежащих работнику $i$.

\subsection{Распределенная оптимизация}
\def\stepsize{\eta}
Основой для решения задачи \eqref{eq:prob} является распределенный градиентный спуск (GD), выполняющий обновления по формуле
\[ x^{k+1} = x^k - \frac{\stepsize^k}{n} \sum \limits_{i=1}^n \nabla f_i(x^k), \]
где $\stepsize^k > 0$ — шаг. Для решения проблем коммуникации в распределенных системах были предложены улучшения, сокращающую размер передаваемых сообщений с помощью операторов сжатия.


\subsection{Оператор сжатия}
 
Под оператором сжатия имеется ввиду (возможно стохастическое) отображение$\cC\colon\R^d\to\R^d$ с некоторыми ограничениями.
Обычно в литературе упоминаются несмещённые операторы сжатия $\cC$ с ограниченным вторым моментом, т.е.

\begin{definition}
Пусть $\zeta \geq 1$. Будем говорить что $\cC\in \mathbb{U}(\zeta)$ если $\cC$ несмещённый (т.е., $\Exp{\cC(x)}=x$  $\forall x$) и если второй момент ограничен 
\begin{equation}
 \Exp{ \twonorm{\cC(x)}^2 } \leq \zeta  \twonorm{x}^2, \qquad \forall x\in\R^d \,.
\end{equation} 

\end{definition}

Далее в работе рассматривается конструирование операторов сжатия на основе малоранговых разложений. 




\bibliographystyle{unsrtnat}
\bibliography{Rebrikov2024LowRankDistFedLearning}

\end{document}